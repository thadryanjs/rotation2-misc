---
toc: true
toc-depth: 3
---

## Goal

To evaluate three different methods for calculating standard error to confirm the correct one for this bootstrap context. Specifically, we will asses weather to use $N$, $K$, or $B$ as defined below.

## Definitions

- $S$:= the number of samples in the experiment.
- $N$:= numbers of observations per sample.
- $B$:= number batches (can be thought of as number of bootstraps).
- $K$:= number of observations per batch ($\frac{N}{K}$).
- $\theta$:= mean of a sample.
- $\theta^{b}$:= mean of a batch (bootstrap).
- $\sigma$:= standard deviation of sample.
- $\sigma_b$:= standard deviation of a batch.
- $R$:= root statistic comparing two means (an empirical mean to a bootstrap).
- $S$:= standard error of a sample ("Outer SE").
- $S_b$:= standard error use in the calculation of $R$ ("Inner SE").
- $\alpha$:= significance level of a comparison.
- $d$:= $1-\alpha$ quantile of a vector of $R$s.
- $Z$:= $\in [N,B,K]$, factor to test the use of in the function.

```{julia}

using Random
using Distributions
using HypothesisTests
using Plots
using PlotThemes; theme(:dracula)
using DataFrames
using CSV
using LaTeXStrings
```

## Parameters

```{julia}

Random.seed!(8675309)

# observations per sample
N = 100000
# number of batches
B = 100
# observations per batch
K = 1000
# number of samples
S = 17
# transparently display what we're using 
println(S, " samples per experiment")
println(N, " observations per sample (N)")
println(B, " batches per subpopulation/bootstrap (B)")
println(K, " observations per batch (K)")
```

## Mathematical Functions

These are the functions we're using for math calculations (as opposed to the programmatic, automation/simulation functions).

### Standard Error $S$

The classical standard error.

```{julia}
 
# compute the standard error
function standard_error(sd1, sd2, N)
    sqrt( ( (sd1^2) + (sd2^2) ) * (N^-1) )
end
```

### Standard Error for a batch $S_b$

The standard error modified for our tests

```{julia}
 
# Z will be either N, M, K
function standard_error_boot(sd1, sd2, Z)
    sqrt( ( (sd1^2) + (sd2^2) ) * (Z^-1) )
end 
```



### Computing $R$

```{julia}
    
# Compute the root statistic
function calc_root_stat(m1, m1b, m2, m2b, se)
    ( (m1b - m2b) - (m1 - m2) ) / se 
end
```


## Simulation Functions

We will write a function to generate a sample.

### generate_sample()

```{julia}
 
# create a sample of normal data
generate_sample(mu, sd, N) = rand(Normal(mu, sd), N)
```

#### Example

```{julia}

# test samples to test/demonstrate the functions on
test_samples = [generate_sample(2, 0.5, N) for i in 1:3]

# inspect the first ten observations of the first of the 3 samples
first(test_samples[1], 10)
```

```{julia}
# make sure the same looks as it should
histogram(test_samples[1])
```


I will now write a separate function to get the information about sample needed to make a comparison to another sample.

### get_batch_info()

This function takes a sample in the form of a vector, and gets bootstrapped samples by dividing the vector into $B$ batches of $K$. It then gets the $\theta$ and $\sigma$ of those batches and thus returns vectors of $B$ $\theta_b$s and $\sigma_b$s. It also gets the sample $\theta$ and $\sigma$, which the bootstrapped values will be compared against. It also names the sample so that any given sample can be looked up again if needed. It returns a dictionary.

```{julia}

### will conduct batch bootstrapping and collect relevant data on the sample
function get_batch_info(sample, sample_name, K)    

    # arrays to store means and SDs
    theta_boots = []
    sd_boots = []

    # this algorithm indexes arrays at...
    # 1-100
    # 101-200
    # 201-300
    # etc
    i = 1
    for x in 1:B
        # the upper limits, ie, 1-100
        j = i  + (K-1)
        batch = sample[i:j]
        ### println("from ", i, " to ", j)
        # collect the figures
        push!(theta_boots, mean(batch))
        push!(sd_boots, std(batch))
        # increase by the batch size the set the next limit
        i += K 
    end
    
    Dict(
        "name" => sample_name,
        "mean" => mean(sample),
        "sd" => std(sample),
        "bootstrap_means" => theta_boots,
        "bootstrap_sds" => sd_boots
    )

end
```

#### Example

```{julia}

test_samples = [
    get_batch_info(test_samples[1], "sample1", K),
    get_batch_info(test_samples[2], "sample2", K),
    get_batch_info(test_samples[3], "sample3", K)
]

test_samples[1]
```

### pairwise_comparison()

This function makes the pairwise comparisons between samples given two samples, it iterates through their bootstrapped $\theta$ s and $\sigma$ s. Thus, it will make $B$ comparisons - the $\theta_s$ against every $\theta_b$, ie for $B=10$:

 - $\theta_s \space$ vs
    - $\theta_{b1}$
    - $\theta_{b2}$
    - $\theta_{b3}$
    - $\theta_{b4}$
    - $\theta_{b5}$
    - $\theta_{b6}$
    - $\theta_{b7}$
    - $\theta_{b8}$
    - $\theta_{b9}$
    - $\theta_{b10}$

The $Z$ is where the experiment takes place. This value is passed to the standard error function for use as the denominator. We will asses the distribution of the test statistic when different values for $Z$ are used. Returns a vector of $R$ stats.

```{julia}

### makes comparisons and returns a vector of R statistics
### z is the N, K, or M we're testing
function pairwise_comparison(sample1, sample2, Z)

    # get relevant sample information
    s1_theta = sample1["mean"]
    s1_theta_boots = sample1["bootstrap_means"]
    s1_sd_boots = sample1["bootstrap_sds"]

    s2_theta = sample2["mean"]
    s2_theta_boots = sample2["bootstrap_means"]
    s2_sd_boots = sample2["bootstrap_sds"]

    # a vector to collect the R statistics
    R_stats = []

    # the number of iterations must match the number of batches, B
    for i in 1:B
        # get the standard error using the current batch sd
        se = standard_error(s1_sd_boots[i], s2_sd_boots[i], Z)
        # pass the sample thetas to compare to this particular bootstrap value
        R = calc_root_stat(s1_theta, s1_theta_boots[i],
                            s2_theta, s2_theta_boots[i], se)
        push!(R_stats, R)
    end
    R_stats
end
```

#### Example

```{julia}

pairwise_comparison(test_samples[1], test_samples[2], N)
```

### run_simulation()

This function actually runs the simulation on a group of samples. It simples takes a list of samples and the $Z$ that we're investigating to pass to the `pairwise_comparison()` function. It returns a dict of information to be passed onto the next function. The key new information it returns is a vector of root statistics, in particular noting the maximum.

```{julia}

### runs pairwise_comparison on a list of samples, skipping redundant comparisons,
function run_simulation(samples, Z)

    # we will make a string describing the comparison we're doing so
    # we can check for redundancies
    comparison = []

    # info we'll return
    r_stats_dict = Dict()

    # for every sample...
    for i in 1:length(samples)
        # for every sample...
        for j in 1:length(samples)
            # don't compare 1 to 1
            if i != j

                # note the comparison for the record
                c = string(i, "_vs_", j)
                # add the comparison we just made it
                push!(comparison, c)
                
                # check for the reverse comparison - don't compare 2-1 if we've
                # done 1-2 ie, don't compare i to j if we've compared j to i
                if string(j, "_vs_", i) in comparison
                    continue # skip this iteration
                end

                # unpack what we'll need
                s1 = samples[i]
                s2 = samples[j]

                m1 = s1["mean"]
                m2 = s2["mean"]

                sd1 = s1["sd"]
                sd2 = s2["sd"]

                ## dev: inefficient but usable for this quick refactor
                ## return the r stat and the other things we will need
                # make the pairwise comparisons of all the sample means
                # against all of the bootstrap/batch means
                r_stats = pairwise_comparison(s1, s2, Z)
                r_stats_dict[c] = Dict(
                    "max_r_stat" => abs(maximum(r_stats)),
                    "r_stats_vector" => r_stats,
                    # we will need these again later
                    "m1" => m1,
                    "m2" => m2,
                    "sd1" => sd1,
                    "sd2" => sd2
                )

            end
        end
    end
    r_stats_dict
end
```

#### Example

```{julia}
run_simulation(test_samples, N)
```

### computeCIs!()

This function takes the results of `run_simulation()` and computes the confidence intervals for each comparison. It just adds on to the object it received. These keeps the `run_simulation()` function a little cleaner.

```{julia}

# this takes the comparison records and adds
function computeCIs!(o, alpha, N)

    # get a vector of all the absolute value of max r stats
    r_stats = [v["max_r_stat"] for (k,v) in o]
    
    # for each comparison
    for (k,v) in o
        
        # note the value we need
        m1 = v["m1"]
        m2 = v["m2"]
        sd1 = v["sd1"]
        sd2 = v["sd2"]

        # compute d and the sample (non-bootstrap) standard error
        d = quantile(r_stats, (1-alpha))
        se = standard_error(sd1, sd2, N)

        # add the new information to the record we have
        v["d"] = d
        v["low"] = round( (m1 - m2) - (d * se), digits = 5) 
        v["obs"] = round( (m1 - m2), digits = 5)
        v["high"] = round( (m1 - m2) + (d * se), digits = 5)

    end
    o
end
```

### get_all_rstats()

A convenience function to get all the $R$ stats (not $R_{max}$ s) in an experiment.

```{julia}

### get all the r stats in an experiment
function get_all_rstats(o)
    all_r_stats = []
    for(k,v) in o
        vec = v["r_stats_vector"]
        for i in vec
            push!(all_r_stats, i)
        end
    end
    all_r_stats
end
```

### get_all_rmax()

A convenience function to get all the $R_{max}$ values in an experiment.

```{julia}

### get all the r max values in an experiment
function get_all_rmax(o)
    all_r_maxes = []
    for (k,v) in o
        push!(all_r_maxes, v["max_r_stat"])
    end
    all_r_maxes
end
```

### get_d()

Gets the $d$ of an experiment. Because it's stored multiple times, it's inefficient but not so much it matters.

```{julia}

### get the d for an experiment
function get_d(o)
    [v["d"] for (k,v) in o][1]
end
```

### describe_experiment()

This function shows numerical and visual results of interest from the experiment passed to it. It prints metrics and returns graphs.

```{julia}

function describe_experiment(sim)

    # get the r stats
    r_stats = get_all_rstats(sim)
    r_max = get_all_rmax(sim)

    # print some QC metrics of interest
    println("d: ", get_d(sim))
    println("Number of R stats: ", length(r_stats))
    println("Number of max R stats: ", length(r_max))
    println("17*8*B: ", 17 * 8 * B)
    println("((S-1)/2 * B): ",  (S * (S-1) /2 ) * B)

    # visualize the r stats
    r_hist = histogram(
        r_stats,
        title = string("Distribution of ", L"R"),
        label = L"R"
    ) 
    r_max_hist = histogram(
        r_max,
        title = string("Distribution of ", L"R_{max}"),
        label = L"$R_{max}$"
    )

    plot(r_hist, r_max_hist, layout = (2,1))
end
```

## The experiment

```{julia}

samples = []
for i in 1:S
    # make a sample
    s = generate_sample(2, 0.5, N)
    # get a report on that sample
    sample_info = get_batch_info(s, string("sample", i), K)
    push!(samples, sample_info)
end

# exit if there is disagreement between the number
# of samples and the desired amount
@assert samples |> length == S
```

### Using $N$

```{julia}

sim_N = run_simulation(samples, N)

sim_N = computeCIs!(sim_N, 0.05, N)

describe_experiment(sim_N)
```

### Using $K$

```{julia}

sim_K = run_simulation(samples, K)

sim_K = computeCIs!(sim_K, 0.05, N)

describe_experiment(sim_K)
```

### Using $B$

```{julia}

sim_B = run_simulation(samples, B)

sim_B = computeCIs!(sim_B, 0.05, N)

describe_experiment(sim_B)
```
