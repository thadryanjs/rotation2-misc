
```{julia}

using Random
using Distributions
using Plots
using PlotThemes; theme(:dracula)
using DataFrames
using CSV
```

## Definitions

 - $N$:= numbers of observations per sample.
 - $B$:= number batches (can be thought of as number of bootstraps).
 - $K$:= number of observations per batch ($\frac{N}{K}$).
 - $\theta$:= mean of a sample.
 - $\theta^{b}$:= mean of a batch (bootstrap).
 - $\sigma$:= standard deviation of sample.
 - $\sigma_b$:= standard deviation of a batch.
 - $R$:= root statistic comparing two means (an empirical mean to a bootstrap).
 - $S$:= standard error of a sample ("Outer SE").
 - $S_b$:= standard error use in the calculation of $R$ ("Inner SE").
 - $\alpha$:= significance level of a comparison.
 - $d$:= $1-\alpha$ quantile of a vector of $R$s.
 - $Z$:= $\in [N,B,K]$, factor to test the use of in the function.


```{julia}

Random.seed!(8675309)

# observations per sample
N = 100000
# number of batches
B = Int(N/100)
# observations per batch
K = Int(N/B)

# number of samples
samples_per_experiment = 17

println(samples_per_experiment, " samples per population")
println(N, " observations per sample (N)")
println(B, " batches per subpopulation/bootstrap (B)")
println(K, " observations per batch (K)")
```

## Mathematical Functions

These are the functions we're using for math calculations (as opposed to the programmatic, automation/simulation functions).

### Standard Error $S$

The classical standard error.

```{julia}
 
# compute the standard error
function standard_error(sd1, sd2, N)
    sqrt( ( (sd1^2) + (sd2^2) ) * (N^-1) )
end
```

### Standard Error for a batch $S_b$

The standard error modified for our tests

```{julia}
 
# Z will be either N, M, K
function standard_error_boot(sd1, sd2, Z)
    sqrt( ( (sd1^2) + (sd2^2) ) * (Z^-1) )
end 
```



### Computing $R$

```{julia}
    
# Compute the root statistic
function calc_root_stat(m1, m1b, m2, m2b, se)
    abs( ( (m1b - m2b) - (m1 - m2) ) /se )
end
```


## Simulation Functions

We will write a function to generate a sample.

```{julia}
 
# create a sample of log-normal data
generate_sample(mu, sd, N) = rand(Normal(mu, sd), N)

# a test sample
samples = [generate_sample(2, 0.5, N) for i in 1:2]

histogram(samples[1])
```


I will now write a separate function to get the information about sample needed to make a comparison.

### Getting batch info

```{julia}

### will conduct batch bootstrapping and collect relevant data on the sample
function get_batch_info(sample, sample_name, K)    

    # arrays to store means and SDs
    theta_boots = []
    sd_boots = []

    # this algorithm indexes arrays at...
    # 1-100
    # 101-200
    # 201-300
    # etc
    i = 1
    for x in 1:B
        # the upper limits, ie, 1-100
        j = i  + (K-1)
        batch = sample[i:j]
        ### println("from ", i, " to ", j)
        # collect the figures
        push!(theta_boots, mean(batch))
        push!(sd_boots, std(batch))
        # increase by the batch size the set the next limit
        i += K 
    end
    
    Dict(
        "name" => sample_name,
        "mean" => mean(sample),
        "sd" => std(sample),
        "bootstrap_means" => theta_boots,
        "bootstrap_sds" => sd_boots
    )

end

sample1 = get_batch_info(samples[1], "sample1", K)
```



Now we have a dictionary containing the information about a samples we need to compare it to another. We will get another sample to compare it to.

```{julia}
sample2 = get_batch_info(samples[2], "sample2", K)
```

### Pairwise comparison

```{julia}

### makes comparisons and returns a vector of R statistics
### z is the N, K, or M we're testing
function pairwise_comparison(sample1, sample2, Z)

    # get relevant sample information
    s1_theta = sample1["mean"]
    s1_theta_boots = sample1["bootstrap_means"]
    s1_sd_boots = sample1["bootstrap_sds"]

    s2_theta = sample2["mean"]
    s2_theta_boots = sample2["bootstrap_means"]
    s2_sd_boots = sample2["bootstrap_sds"]

    # a vector to collect the R statistics
    R_stats = []

    # the number of iterations must match the number of batches, K
    for i in 1:K
        # get the standard error using the current batch sd
        se = standard_error(s1_sd_boots[i], s2_sd_boots[i], Z)
        
        R = calc_root_stat(s1_theta, s1_theta_boots[i],
                            s2_theta, s2_theta_boots[i], se)
        push!(R_stats, R)
    end
    
    maximum(R_stats)

end
```

```{julia}

R_stat_N = pairwise_comparison(sample1, sample2, N)
```

## The simulation

We can now create a fully-sized dataset for the simulation.

```{julia}

samples = []

for i in 1:samples_per_experiment
    s = generate_sample(2, 0.5, N)
    sr = get_batch_info(s, string("sample", i), K)
    push!(samples, sr)
end

println("Number of samples: ", length(samples))
samples[1:3]
```

```{julia}

function run_simulation(samples, Z)

    comparison = []

    r_stats_dict = Dict()

    for i in 1:length(samples)
        for j in 1:length(samples)
            if i != j

                # note the comparison for the record
                c = string(i, "_vs_", j)
                
                # don't compare 2-1 if we've done 1-2
                if string(j, "_vs_", i) in comparison
                    continue
                end

                # unpack what we'll need
                s1 = samples[i]
                s2 = samples[j]

                m1 = s1["mean"]
                m2 = s2["mean"]

                sd1 = s1["sd"]
                sd2 = s2["sd"]

                # dev: inefficient but usable for this quick refactor
                # return the r stat and the other things we will need
                r_stats_dict[c] = Dict(
                    "r_stat" => pairwise_comparison(s1, s2, Z),
                    "m1" => m1,
                    "m2" => m2,
                    "sd1" => sd1,
                    "sd2" => sd2
                )

            end
        end
    end

    r_stats_dict

end
```

What about the confidence intervals for each comparison in this simulation?

```{julia}

# this takes the comparison records and adds
function computeCIs!(o, alpha, N)

    r_stats = [v["r_stat"] for (k,v) in o]
    
    for (k,v) in o
        
        m1 = v["m1"]
        m2 = v["m2"]
        sd1 = v["sd1"]
        sd2 = v["sd2"]

        d = quantile(r_stats, (1-alpha))
        se = standard_error(sd1, sd2, N)

        v["low"] = (m1 - m2) - (d * se)
        v["obs"] = (m1 - m2)
        v["high"] = (m1 - m2) + (d * se)

    end
    o
end

# get the r stats for our experiment
get_r_stats(o) = [v["r_stat"] for (k,v) in o]
```

### Using N

```{julia}


sim_N = run_simulation(samples, N)

sim_N = computeCIs!(sim_N, 0.05, N)

r_stats_N = get_r_stats(sim_N)

r_stats_N |> histogram
```

### Using K

```{julia}

sim_K = run_simulation(samples, K)

sim_K = computeCIs!(sim_K, 0.05, N)

r_stats_K = get_r_stats(sim_K)

r_stats_K |> histogram
```


### Using B


```{julia}

sim_B = run_simulation(samples, N)

sim_B = computeCIs!(sim_N, 0.05, N)

r_stats_B = get_r_stats(sim_N)

r_stats_B |> histogram
```


```{julia}
println("End...")
```

```{julia}

``` 


# TODO
    -  Get one R stat per test (largest j>j')